{
  "hash": "bf34db1937341d38129809c5a5354928",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Using Genetic Algorithms to Optimize Feature Selection in Regression Modeling\"\nauthor: \"Oluwaseun Daniel Fowotade\"\ndate: \"12/20/2024\"\ndescription: \"Using Genetic Algorithms to optimize feature selection in regression modeling.\"\nformat:\n  html:\n    toc: true\n    toc-location: left\n    code-fold: true\n    highlight-style: tango\nexecute:\n  warning: false\n  message: false\njupyter: ir\ncategories:\n  - Machine Learning\n  - Genetic Algorithms\n  - Feature Selection\n  - Statistics\n  - Regression\n---\n\n![](/Assets/Images/GA.jpeg)\n\n# Overview of Genetic Algorithm\n\nFeature selection is a crucial step in building efficient machine learning models, especially in the context of regression. It involves selecting a subset of relevant features from a larger set, aiming to improve model performance, reduce overfitting, and increase interpretability. Traditional feature selection methods, such as backward elimination or stepwise regression, can struggle when dealing with high-dimensional or complex data. These methods may fail to capture nonlinear relationships between features and can be computationally expensive for large datasets.\n\nGenetic algorithms (GAs), inspired by the principles of natural selection, offer a promising approach to overcome these challenges. First proposed by John Holland in 1975, GAs are heuristic optimization techniques that simulate the process of natural evolution. In a GA, a population of candidate solutions evolves over generations using genetic operations like selection, crossover, and mutation.\n\nThis project applies GAs to feature selection in regression tasks, where the goal is to minimize prediction error and avoid overfitting. By leveraging the power of genetic evolution, the GA can efficiently explore large, high-dimensional spaces and identify the most relevant features for building robust predictive models.\n\n# Methodology of Genetic Algorithm\n\nThe application of genetic algorithms to feature selection in regression models involves several key steps:\n\n## Chromosome Encoding and Representation\n\nEach candidate solution is represented as a chromosome, a binary vector where each bit represents the inclusion (`1`) or exclusion (`0`) of a feature. If a dataset contains (C) features, the chromosome length is (C), and each gene corresponds to a specific feature.\n\nFor example: (1, 0, 1, 0, 1) Features 1, 3, and 5 selected, while 2 and 4 are excluded.\n\n## Population Initialization\n\nThe algorithm starts by initializing a population of chromosomes randomly. The size of the population, (P), is critical as it influences the diversity of solutions. A larger population ensures more diversity, preventing premature convergence, while a smaller population might speed up the process but risk missing better solutions. Typically, the population size is chosen so that:\n\n\\[ C \\leq P \\leq 2C \\]\n\nwhere (C) is the number of features.\n\n## Selection\n\nThe selection step involves choosing individuals from the population based on their fitness to act as parents for the next generation. The fitness of a chromosome is determined by evaluating its associated feature subset in a regression model, typically using the mean squared error (MSE) as the performance metric. Tournament selection is often used, where a subset of chromosomes is randomly chosen, and the one with the best fitness is selected as a parent.\n\n## Crossover\n\nOnce parents are selected, they undergo genetic operations to create offspring. Crossover involves swapping parts of two parent chromosomes to produce offspring. A common method is single-point crossover, where a random crossover point is chosen, and the genes before and after this point are exchanged between the parents.\n\n## Mutation\n\nMutation introduces small random changes to maintain genetic diversity and avoid local optima. For instance, a mutation may flip the value of a random bit in the chromosome (e.g., changing `0` to `1` or vice versa). The mutation rate is usually kept low (e.g., 1% per generation) to ensure diversity without disrupting good solutions.\n\n## Fitness Evaluation\n\nEach chromosome is evaluated using a regression model trained on the selected features. The fitness function is:\n\n$$\n\\text{Fitness}(\\vartheta) = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n$$\n\nwhere $(y_i)$ are actual values and \\\\( \\\\hat{y}\\_i \\\\) are predictions.\n\n## Elitism\n\nTo ensure that the best solutions are preserved, elitism is applied. This technique guarantees that the best-performing chromosome from the current generation is directly passed to the next generation without any changes. This helps prevent the loss of high-quality solutions during the evolution process.\n\n## Termination\n\nThe algorithm runs for a set number of generations or until a stopping criterion is met. Common criteria include reaching a maximum number of generations (e.g., 100) or achieving a satisfactory fitness level. Once the algorithm terminates, the best chromosome found represents the optimal feature subset.\n\n# Final Model\n\nThe genetic algorithm (GA) was implemented to identify an optimal subset of features for the classification task in the Sonar dataset. Over 89 iterations, the algorithm demonstrated consistent improvement in fitness, ultimately achieving a best fitness value of 0.3077. This fitness value, defined as (1 - \\text{accuracy}), indicates a corresponding predictive accuracy of 69.23% on the training set. The selected feature subset was designed to maximize classification performance while minimizing redundancy and over-fitting.\n\nThe GA selected the following features from the original set of 60 predictors:\n\nSelected features: 1, 2, 3, 4, 5, 6, 7, 16, 17, 18, 19, 22, 24, 25, 28, 29, 30, 33, 37, 39, 41, 43, 47, 49, 51, 52, 58.\n\nThis reduced set of features was used to train a k-Nearest Neighbors (k-NN) classifier with (k = 3), selected based on a grid search for hyperparameter optimization. The training utilized 5-fold cross-validation to ensure model robustness and generalizability. On the test set, the following confusion matrix was obtained:\n\n::: {#26e0e4c3 .cell execution_count=1}\n``` {.r .cell-code}\n# Create confusion matrix as a data frame\nconf_matrix <- data.frame(\n  \"Actual\" = c(\"Actual M\", \"Actual R\"),\n  \"Predicted M\" = c(26, 7),\n  \"Predicted R\" = c(10, 19)\n)\n\n#kable to create a nicely formatted table\nknitr::kable(conf_matrix, caption = \"Confusion Matrix: Predicted vs Actual\", align = \"c\")\n```\n\n::: {.cell-output .cell-output-display}\n```\n\n\nTable: Confusion Matrix: Predicted vs Actual\n\n|  Actual  | Predicted.M | Predicted.R |\n|:--------:|:-----------:|:-----------:|\n| Actual M |     26      |     10      |\n| Actual R |      7      |     19      |\n```\n:::\n:::\n\n\nFrom the confusion matrix, the following performance metrics were computed:\n\n-   **Accuracy**: 72.58%\n-   **Sensitivity (True Positive Rate for Mines)**: 78.79%\n-   **Specificity (True Negative Rate for Rocks)**: 65.52%\n-   **Kappa Statistic**: 0.4458\n-   **Balanced Accuracy**: 72.15%\n\nThe k-NN classifier displayed a solid ability to correctly classify mines, as indicated by the relatively high sensitivity of 78.79%. However, the specificity of 65.52% suggests room for improvement in accurately identifying rocks, particularly in cases with overlapping feature distributions.\n\n# Conclusion\n\nThe application of a genetic algorithm (GA) for feature selection in the Sonar dataset successfully reduced the dimensional from 60 to 27 features, enabling efficient model training while maintaining predictive performance. The GA’s ability to explore a vast and complex search space was instrumental in identifying relevant features that traditional methods might have overlooked.\n\nThe k-NN classifier trained on the selected features achieved an accuracy of 72.58% on the test set. However, there are several areas for improvement:\n\n- ##  **Population Size**: A larger population size could enhance the diversity of candidate solutions, preventing premature convergence to suboptimal solutions.\n-   **Mutation Rate**: Adjusting the mutation rate could introduce more variation in the search space. For example, increasing the mutation rate in earlier generations might explore more diverse feature subsets, while reducing it in later generations could refine promising solutions.\n-   **Crossover Strategy**: Employing advanced crossover techniques, such as uniform or multi-point crossover, could enhance the recombination of parent solutions, potentially leading to better offspring.\n-   **Termination Criteria**: Allowing the GA to run for additional generations or introducing a dynamic stopping criterion based on convergence rates could yield more refined feature subsets.\n-   **Fitness Function**: Experimenting with alternative fitness functions, such as F1-score or balanced accuracy, could prioritize models that address class imbalance more effectively.\n\nBy fine-tuning these parameters, the GA could further enhance its ability to select the most discriminating features, improving classification accuracy and robustness.\n\nThis analysis underscores the effectiveness of genetic algorithms in tackling complex feature selection challenges, particularly for high-dimensional data sets with potential nonlinear relationships among features. Future research could integrate hybrid optimization techniques, combining GAs with other meta heuristic methods like particle swarm optimization or simulated annealing, to achieve even better results.\n\nThe results illustrate that intelligent feature selection not only reduces computational costs but also leads to models that are easier to interpret and maintain, making GAs a valuable tool in the arsenal of modern machine learning techniques.\n\n# References\n\n-   Kaya, Y. (2018). Comparison of Using the Genetic Algorithm and Cuckoo Search for Feature Selection.\n-   Raymer, M.L. et al. (2000). Dimensionality reduction using genetic algorithms. IEEE Transactions on Evolutionary Computation.\n-   Bindu, M.G. & Sabu, M.K. (2020). A Hybrid Feature Selection Approach Using Artificial Bee Colony and Genetic Algorithm.\n-   Givens, G.H. & Hoeting, J.A. (2012). Computational Statistics, Vol. 703. John Wiley & Sons.\n\n# Appendix\n\n## Genetic Algorithm Implementation on Sonar Dataset\n\n### Loading Required Libraries and Data\n\n::: {#1be747ea .cell execution_count=2}\n``` {.r .cell-code}\nlibrary(GA)\nlibrary(caret)\nlibrary(mlbench)\n\n# Load the Sonar dataset\ndata(Sonar)\nSonar$Class <- as.factor(Sonar$Class) # Ensure the target variable is a factor\n\n# Split data into training and testing sets\nset.seed(123) # For reproducibility\ntrainIndex <- createDataPartition(Sonar$Class, p = 0.7, list = FALSE)\ntrainData <- Sonar[trainIndex, ]\ntestData <- Sonar[-trainIndex, ]\n```\n:::\n\n\n### Defining the Fitness Function for Feature Selection\n\n::: {#e195d5de .cell execution_count=3}\n``` {.r .cell-code}\n# Define the fitness function\nfitness_function <- function(x) {\n  selected_features <- which(x == 1) # Indices of selected features\n  \n  # If no features are selected, assign a large fitness value (penalty)\n  if (length(selected_features) == 0) return(Inf)\n  \n  # Subset the training data with the selected features\n  selected_train <- trainData[, c(selected_features, 61)] # Include 'Class'\n  \n  # Train a k-NN model\n  ctrl <- trainControl(method = \"cv\", number = 5)\n  knn_model <- train(Class ~ ., data = selected_train, method = \"knn\", \n                     trControl = ctrl, tuneGrid = data.frame(k = 1))\n  \n  # Return fitness value (1 - accuracy to minimize error)\n  fitness_value <- 1 - max(knn_model$results$Accuracy)\n  return(fitness_value)\n}\n```\n:::\n\n\n```         \n```\n\n## Running the Genetic Algorithm\n\n::: {#ca408a0a .cell execution_count=4}\n``` {.r .cell-code}\nset.seed(123) # For reproducibility\n\n# Run the Genetic Algorithm\nga_model <- ga(\n  type = \"binary\",\n  fitness = fitness_function,\n  nBits = ncol(Sonar) - 1, # 60 features\n  popSize = 50,\n  maxiter = 100,\n  pcrossover = 0.8,\n  pmutation = 0.1,\n  elitism = 2,\n  run = 50,\n  monitor = TRUE\n)\n\n# Summary of GA results\nsummary(ga_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGA | iter = 1 | Mean = 0.1680522 | Best = 0.2391461\nGA | iter = 2 | Mean = 0.1735294 | Best = 0.2391461\nGA | iter = 3 | Mean = 0.1759514 | Best = 0.2391461\nGA | iter = 4 | Mean = 0.1813278 | Best = 0.2391461\nGA | iter = 5 | Mean = 0.1757665 | Best = 0.2393924\nGA | iter = 6 | Mean = 0.1828765 | Best = 0.2540230\nGA | iter = 7 | Mean = 0.1781392 | Best = 0.2540230\nGA | iter = 8 | Mean = 0.1847412 | Best = 0.2540230\nGA | iter = 9 | Mean = 0.1869241 | Best = 0.2540230\nGA | iter = 10 | Mean = 0.1886102 | Best = 0.2540230\nGA | iter = 11 | Mean = 0.1939878 | Best = 0.2540230\nGA | iter = 12 | Mean = 0.1903678 | Best = 0.2540230\nGA | iter = 13 | Mean = 0.1993767 | Best = 0.2540230\nGA | iter = 14 | Mean = 0.1980716 | Best = 0.2540230\nGA | iter = 15 | Mean = 0.1937833 | Best = 0.2555829\nGA | iter = 16 | Mean = 0.1926673 | Best = 0.2555829\nGA | iter = 17 | Mean = 0.1877202 | Best = 0.2555829\nGA | iter = 18 | Mean = 0.185753 | Best = 0.260000\nGA | iter = 19 | Mean = 0.1842003 | Best = 0.2600000\nGA | iter = 20 | Mean = 0.1971665 | Best = 0.2600000\nGA | iter = 21 | Mean = 0.1958624 | Best = 0.2660755\nGA | iter = 22 | Mean = 0.1957360 | Best = 0.2660755\nGA | iter = 23 | Mean = 0.2017741 | Best = 0.2740230\nGA | iter = 24 | Mean = 0.196908 | Best = 0.274023\nGA | iter = 25 | Mean = 0.1948959 | Best = 0.2740230\nGA | iter = 26 | Mean = 0.1974177 | Best = 0.2740230\nGA | iter = 27 | Mean = 0.2007126 | Best = 0.2740230\nGA | iter = 28 | Mean = 0.2018808 | Best = 0.2740230\nGA | iter = 29 | Mean = 0.2000319 | Best = 0.2740230\nGA | iter = 30 | Mean = 0.208091 | Best = 0.274023\nGA | iter = 31 | Mean = 0.2107612 | Best = 0.2740230\nGA | iter = 32 | Mean = 0.2085021 | Best = 0.2740230\nGA | iter = 33 | Mean = 0.2049212 | Best = 0.2740230\nGA | iter = 34 | Mean = 0.2075839 | Best = 0.2740230\nGA | iter = 35 | Mean = 0.2106174 | Best = 0.2740230\nGA | iter = 36 | Mean = 0.2072158 | Best = 0.2740230\nGA | iter = 37 | Mean = 0.2078000 | Best = 0.2741379\nGA | iter = 38 | Mean = 0.2109856 | Best = 0.2741379\nGA | iter = 39 | Mean = 0.2107938 | Best = 0.2741379\nGA | iter = 40 | Mean = 0.2141228 | Best = 0.2741379\nGA | iter = 41 | Mean = 0.2118667 | Best = 0.2952381\nGA | iter = 42 | Mean = 0.2072361 | Best = 0.2952381\nGA | iter = 43 | Mean = 0.2125681 | Best = 0.2952381\nGA | iter = 44 | Mean = 0.2125563 | Best = 0.2952381\nGA | iter = 45 | Mean = 0.2114686 | Best = 0.2952381\nGA | iter = 46 | Mean = 0.2161307 | Best = 0.2952381\nGA | iter = 47 | Mean = 0.2289176 | Best = 0.3212644\nGA | iter = 48 | Mean = 0.2328535 | Best = 0.3212644\nGA | iter = 49 | Mean = 0.2339862 | Best = 0.3212644\nGA | iter = 50 | Mean = 0.2373701 | Best = 0.3212644\nGA | iter = 51 | Mean = 0.2418594 | Best = 0.3212644\nGA | iter = 52 | Mean = 0.2476099 | Best = 0.3291133\nGA | iter = 53 | Mean = 0.2477376 | Best = 0.3291133\nGA | iter = 54 | Mean = 0.2437938 | Best = 0.3291133\nGA | iter = 55 | Mean = 0.2490690 | Best = 0.3294253\nGA | iter = 56 | Mean = 0.2584273 | Best = 0.3294253\nGA | iter = 57 | Mean = 0.2577984 | Best = 0.3294253\nGA | iter = 58 | Mean = 0.2609612 | Best = 0.3294253\nGA | iter = 59 | Mean = 0.2552857 | Best = 0.3294253\nGA | iter = 60 | Mean = 0.2585452 | Best = 0.3294253\nGA | iter = 61 | Mean = 0.2510910 | Best = 0.3294253\nGA | iter = 62 | Mean = 0.2670933 | Best = 0.3345977\nGA | iter = 63 | Mean = 0.2699327 | Best = 0.3636453\nGA | iter = 64 | Mean = 0.2662998 | Best = 0.3636453\nGA | iter = 65 | Mean = 0.2649008 | Best = 0.3636453\nGA | iter = 66 | Mean = 0.2650059 | Best = 0.3636453\nGA | iter = 67 | Mean = 0.2769990 | Best = 0.3636453\nGA | iter = 68 | Mean = 0.2793964 | Best = 0.3636453\nGA | iter = 69 | Mean = 0.2733383 | Best = 0.3970115\nGA | iter = 70 | Mean = 0.2746039 | Best = 0.3970115\nGA | iter = 71 | Mean = 0.2773366 | Best = 0.3970115\nGA | iter = 72 | Mean = 0.2772378 | Best = 0.3970115\nGA | iter = 73 | Mean = 0.2705379 | Best = 0.3970115\nGA | iter = 74 | Mean = 0.2710571 | Best = 0.3970115\nGA | iter = 75 | Mean = 0.2703310 | Best = 0.3970115\nGA | iter = 76 | Mean = 0.2769672 | Best = 0.3970115\nGA | iter = 77 | Mean = 0.2862611 | Best = 0.3970115\nGA | iter = 78 | Mean = 0.2788966 | Best = 0.3970115\nGA | iter = 79 | Mean = 0.2770125 | Best = 0.3970115\nGA | iter = 80 | Mean = 0.2758213 | Best = 0.3970115\nGA | iter = 81 | Mean = 0.2805373 | Best = 0.3970115\nGA | iter = 82 | Mean = 0.2811612 | Best = 0.3970115\nGA | iter = 83 | Mean = 0.2812105 | Best = 0.3970115\nGA | iter = 84 | Mean = 0.2845800 | Best = 0.3970115\nGA | iter = 85 | Mean = 0.2736368 | Best = 0.3970115\nGA | iter = 86 | Mean = 0.2713576 | Best = 0.3970115\nGA | iter = 87 | Mean = 0.2734690 | Best = 0.3970115\nGA | iter = 88 | Mean = 0.2683113 | Best = 0.3970115\nGA | iter = 89 | Mean = 0.2611491 | Best = 0.3970115\nGA | iter = 90 | Mean = 0.2540072 | Best = 0.3970115\nGA | iter = 91 | Mean = 0.2643452 | Best = 0.3970115\nGA | iter = 92 | Mean = 0.2678791 | Best = 0.3970115\nGA | iter = 93 | Mean = 0.2670039 | Best = 0.3970115\nGA | iter = 94 | Mean = 0.2731304 | Best = 0.3970115\nGA | iter = 95 | Mean = 0.2646749 | Best = 0.3970115\nGA | iter = 96 | Mean = 0.2710936 | Best = 0.3970115\nGA | iter = 97 | Mean = 0.2621143 | Best = 0.3970115\nGA | iter = 98 | Mean = 0.2651343 | Best = 0.3970115\nGA | iter = 99 | Mean = 0.2701452 | Best = 0.3970115\nGA | iter = 100 | Mean = 0.2667343 | Best = 0.3970115\n```\n:::\n\n::: {.cell-output .cell-output-display}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre>── <span class=\"ansi-bold\">Genetic Algorithm</span> ─────────────────── \nGA settings: \nType                  =  binary \nPopulation size       =  50 \nNumber of generations =  100 \nElitism               =  2 \nCrossover probability =  0.8 \nMutation probability  =  0.1 \nGA results: \nIterations             = 100 \nFitness function value = 0.3970115 \nSolution = \n     x1 x2 x3 x4 x5 x6 x7 x8 x9 x10  ...  x59 x60\n[1,]  0  1  0  0  1  0  1  0  0   1         1   1</pre>\n```\n:::\n\n:::\n:::\n\n\n### Selected Features and Final Model Evaluation\n\n::: {#d97ca8f2 .cell execution_count=5}\n``` {.r .cell-code}\n# Extract best chromosome (selected features)\nbest_chromosome <- ga_model@solution\nselected_features <- which(best_chromosome == 1)\n\n# Train the final k-NN model using selected features\nselected_train <- trainData[, c(selected_features, 61)]\nselected_test <- testData[, c(selected_features, 61)]\n\nfinal_knn_model <- train(Class ~ ., data = selected_train, method = \"knn\",\n                         trControl = trainControl(method = \"cv\", number = 5),\n                         tuneGrid = data.frame(k = 1))\n\n# Make predictions and evaluate\npredictions <- predict(final_knn_model, newdata = selected_test)\nconf_matrix <- confusionMatrix(predictions, selected_test$Class)\n\n# Print results\nconf_matrix\n```\n\n::: {.cell-output .cell-output-display}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  M  R\n         M 23 13\n         R 10 16\n                                          \n               Accuracy : 0.629           \n                 95% CI : (0.4969, 0.7484)\n    No Information Rate : 0.5323          \n    P-Value [Acc > NIR] : 0.0801          \n                                          \n                  Kappa : 0.2503          \n                                          \n Mcnemar's Test P-Value : 0.6767          \n                                          \n            Sensitivity : 0.6970          \n            Specificity : 0.5517          \n         Pos Pred Value : 0.6389          \n         Neg Pred Value : 0.6154          \n             Prevalence : 0.5323          \n         Detection Rate : 0.3710          \n   Detection Prevalence : 0.5806          \n      Balanced Accuracy : 0.6243          \n                                          \n       'Positive' Class : M               \n                                          \n```\n:::\n:::\n\n\n### Comparing Performance with All Features\n\n::: {#b03c2b38 .cell execution_count=6}\n``` {.r .cell-code}\n# Train a model using all features\nmodel_all_features <- train(Class ~ ., data = trainData, method = \"knn\", \n                            trControl = trainControl(method = \"cv\", number = 5), \n                            tuneGrid = data.frame(k = 1))\n\n# Evaluate model on the test set\npredictions_all <- predict(model_all_features, newdata = testData)\nconf_matrix_all <- confusionMatrix(predictions_all, testData$Class)\n\n# Comparison\ncat(\"Accuracy with Selected Features: \", conf_matrix$overall[\"Accuracy\"], \"\\n\")\ncat(\"Accuracy with All Features: \", conf_matrix_all$overall[\"Accuracy\"], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy with Selected Features:  0.6290323 \nAccuracy with All Features:  0.7903226 \n```\n:::\n:::\n\n\n### Visualizing the GA Progress and Results\n\n::: {#7d4e495f .cell execution_count=7}\n``` {.r .cell-code}\nlibrary(ggplot2)\n\n# Plot GA optimization progress\nplot(ga_model, main = \"GA Optimization Progress\", col = \"blue\")\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=420 height=420}\n:::\n:::\n\n\n## Selected Features via Genetic Algorithm\n\n::: {#1229e439 .cell execution_count=8}\n``` {.r .cell-code}\nlibrary(ggplot2)\n\n# Ensure best_chromosome is a numeric vector\nbest_chromosome <- as.numeric(best_chromosome)\n\n# Create a data frame for visualization\nselected_features_df <- data.frame(\n  Feature = factor(1:length(best_chromosome)),  # Feature indices\n  Selected = best_chromosome                    # Binary selection (1 = selected, 0 = not selected)\n)\n\n# Ensure there are no missing or incorrect values\nprint(head(selected_features_df))  # Check structure\n\n#  visualization\nggplot(selected_features_df, aes(x = Feature, y = Selected, fill = as.factor(Selected))) +\n  geom_bar(stat = \"identity\", color = \"black\", width = 0.7) +  # Adjust bar width\n  scale_fill_manual(values = c(\"0\" = \"gray85\", \"1\" = \"#2E8B57\"), name = \"Selected\") +  # Soft gray & deep green\n  labs(title = \" Selected Features via Genetic Algorithm\",\n       subtitle = \"Selected features (1) vs. Non-selected features (0)\",\n       x = \"Feature Index\", \n       y = \"Selection Status\") +  # Clearer label\n  theme_minimal(base_size = 14) +  # Improve readability\n  theme(\n    plot.title = element_text(face = \"bold\", size = 18, hjust = 0.5), # Bold, centered title\n    plot.subtitle = element_text(size = 14, hjust = 0.5, color = \"gray40\"), # Styled subtitle\n    legend.position = \"top\",\n    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 10), # Adjust rotation & spacing\n    axis.text.y = element_text(size = 12),  # Make y-axis text larger\n    panel.grid.major.x = element_blank(),  # Remove unnecessary grid lines\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_line(color = \"gray90\", linetype = \"dashed\")  # Subtle dashed grid lines\n  ) +\n  scale_x_discrete(breaks = seq(1, length(best_chromosome), by = 2)) +  # Reduce x-axis label crowding\n  scale_y_continuous(breaks = c(0, 1), labels = c(\"Not Selected\", \"Selected\"))  # Custom y-axis labels\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Feature Selected\n1       1        0\n2       2        1\n3       3        0\n4       4        0\n5       5        1\n6       6        0\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-2.png){width=420 height=420}\n:::\n:::\n\n\n### Final Confusion Matrix and Performance Metrics\n\n::: {#7d8d10c6 .cell execution_count=9}\n``` {.r .cell-code}\nlibrary(ggplot2)\n\n# Create a data frame for the confusion matrix\nconf_matrix_df <- data.frame(\n  Prediction = c(\"M\", \"M\", \"R\", \"R\"),  # Predicted classes\n  Reference = c(\"M\", \"R\", \"M\", \"R\"),  # Actual classes\n  Count = c(26, 10, 7, 19)            # Counts from the confusion matrix\n)\n\n# Plot the confusion matrix\nggplot(conf_matrix_df, aes(x = Reference, y = Prediction, fill = Count)) +\n  geom_tile(color = \"black\") +\n  geom_text(aes(label = Count), size = 6, color = \"white\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"blue\") +\n  labs(\n    title = \"Confusion Matrix\",\n    subtitle = \"Model Predictions vs. Actual Classes\",\n    x = \"Actual Class\",\n    y = \"Predicted Class\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    axis.text = element_text(size = 12),\n    axis.title = element_text(size = 14),\n    legend.position = \"none\"\n  )\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=420 height=420}\n:::\n:::\n\n\n::: {#9d8e8398 .cell execution_count=10}\n``` {.r .cell-code}\n# Confusion matrix for the model with selected features\nconf_matrix$table\n\n# Accuracy, Sensitivity, and Specificity\nconf_matrix$overall[\"Accuracy\"]\nconf_matrix$byClass[c(\"Sensitivity\")]\nconf_matrix$byClass[c( \"Specificity\")]\n```\n\n::: {.cell-output .cell-output-display}\n```\n          Reference\nPrediction  M  R\n         M 23 13\n         R 10 16\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<strong>Accuracy:</strong> 0.629032258064516\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<strong>Sensitivity:</strong> 0.696969696969697\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<strong>Specificity:</strong> 0.551724137931035\n```\n:::\n:::\n\n\n## Save Result\n\n::: {#6b9502d7 .cell execution_count=11}\n``` {.r .cell-code}\n# Save GA results \nsaveRDS(ga_model, \"ga_model.rds\") # Save the GA model\nsaveRDS(final_knn_model, \"final_knn_model.rds\") # Save the final k-NN model\nsaveRDS(conf_matrix, \"conf_matrix.rds\") # Save the confusion matrix\n\ncat(\"Results have been saved as .rds files in the working directory.\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResults have been saved as .rds files in the working directory.\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}